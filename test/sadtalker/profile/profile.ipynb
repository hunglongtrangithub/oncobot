{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 cpu\n",
      "torch.float32 cpu\n",
      "tensor([ -0.0000,   0.5068,   2.9216,  -1.5335,   3.3608,  12.0714,   6.3862,\n",
      "         -7.4236,   0.9255,   7.3898,  22.2197,  11.5649,   0.0366, -25.1163,\n",
      "        -21.7017])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "idx_tensor = torch.arange(15)\n",
    "tensor = torch.randn(15)\n",
    "print(idx_tensor.dtype, idx_tensor.device)  \n",
    "print(tensor.dtype, tensor.device)\n",
    "print(idx_tensor * tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cuda:0\n",
      "torch.float16 cuda:0\n",
      "tensor([-0.6114, -0.0056, -0.0560, -0.3981, -1.2259,  0.7283, -0.0042, -0.1142,\n",
      "        -2.0781,  0.8457, -0.1566,  0.1703,  1.0486, -0.0755, -0.2323],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "tensor1 = torch.randn(15).type(torch.float32).to(\"cuda\")\n",
    "tensor2 = torch.randn(15).type(torch.float16).to(\"cuda\")\n",
    "print(tensor1.dtype, tensor1.device)\n",
    "print(tensor2.dtype, tensor2.device)\n",
    "print(tensor1 * tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 15, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(os.path.abspath(\"\")).parents[2]))\n",
    "\n",
    "from src.sadtalker.src.facerender.modules.make_animation import keypoint_transformation\n",
    "\n",
    "batch_size = 50\n",
    "kp_canonical = {\n",
    "    \"value\": torch.randn(batch_size, 15, 3).type(torch.float16),\n",
    "}\n",
    "he_source = {\n",
    "    \"yaw\": torch.randn(batch_size, 66).type(torch.float16),\n",
    "    \"pitch\": torch.randn(batch_size, 66).type(torch.float16),\n",
    "    \"roll\": torch.randn(batch_size, 66).type(torch.float16),\n",
    "    \"t\": torch.randn(batch_size, 3).type(torch.float16),\n",
    "    \"exp\": torch.randn(batch_size, 45).type(torch.float16),\n",
    "}\n",
    "\n",
    "kp_source = keypoint_transformation(kp_canonical, he_source)\n",
    "print(kp_source[\"value\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00101470947265625 torch.Size([50, 15, 3, 3])\n",
      "0.28836512565612793 torch.Size([50, 15, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(os.path.abspath(\"\")).parents[2]))\n",
    "from src.sadtalker.src.facerender.modules.util import make_coordinate_grid, kp2gaussian\n",
    "\n",
    "batch_size = 50\n",
    "kp_driving = {\n",
    "    \"value\": torch.randn(batch_size, 15, 3).type(torch.float32).to(\"cuda:0\"),\n",
    "}\n",
    "start = time.time()\n",
    "identity_grid = make_coordinate_grid((batch_size, 15, 3))\n",
    "print(time.time() - start, identity_grid.shape)\n",
    "\n",
    "spatial_size = torch.Size([16, 64, 64])\n",
    "start = time.time()\n",
    "gaussian_driving = kp2gaussian(kp_driving, spatial_size=spatial_size, kp_variance=0.01)\n",
    "print(time.time() - start, gaussian_driving.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import (\n",
    "    profile,\n",
    "    record_function,\n",
    "    ProfilerActivity,\n",
    "    tensorboard_trace_handler,\n",
    ")\n",
    "\n",
    "\n",
    "def make_coordinate_grid(spatial_size: tuple[int, int, int]) -> torch.Tensor:\n",
    "    d, h, w = spatial_size\n",
    "    x = torch.arange(w)\n",
    "    y = torch.arange(h)\n",
    "    z = torch.arange(d)\n",
    "\n",
    "    x = 2 * (x / (w - 1)) - 1\n",
    "    y = 2 * (y / (h - 1)) - 1\n",
    "    z = 2 * (z / (d - 1)) - 1\n",
    "\n",
    "    yy = y.view(1, -1, 1).repeat(d, 1, w)\n",
    "    xx = x.view(1, 1, -1).repeat(d, h, 1)\n",
    "    zz = z.view(-1, 1, 1).repeat(1, h, w)\n",
    "\n",
    "    meshed = torch.cat([xx.unsqueeze_(3), yy.unsqueeze_(3), zz.unsqueeze_(3)], 3)\n",
    "    return meshed\n",
    "\n",
    "\n",
    "def kp2gaussian(kp, spatial_size, kp_variance):\n",
    "    \"\"\"\n",
    "    Transform a keypoint into gaussian like representation\n",
    "    \"\"\"\n",
    "    mean = kp[\"value\"]\n",
    "    # TEST: spatial_size torch.Size([16, 64, 64])\n",
    "    # print(\"spatial_size\", spatial_size)\n",
    "    coordinate_grid = make_coordinate_grid(spatial_size)\n",
    "    number_of_leading_dimensions = len(mean.shape) - 1\n",
    "    shape = (1,) * number_of_leading_dimensions + coordinate_grid.shape\n",
    "    coordinate_grid = coordinate_grid.view(*shape)\n",
    "    repeats = mean.shape[:number_of_leading_dimensions] + (1, 1, 1, 1)\n",
    "    coordinate_grid = coordinate_grid.repeat(*repeats)\n",
    "\n",
    "    # Preprocess kp shape\n",
    "    shape = mean.shape[:number_of_leading_dimensions] + (1, 1, 1, 3)\n",
    "    mean = mean.view(*shape)\n",
    "\n",
    "    mean_sub = coordinate_grid.to(mean.device, non_blocking=True) - mean\n",
    "\n",
    "    out = torch.exp(-0.5 * (mean_sub**2).sum(-1) / kp_variance)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "batch_size = 30\n",
    "kp_driving = {\"value\": torch.randn(batch_size, 15, 3).type(torch.float32).to(\"cuda:0\")}\n",
    "spatial_size = (16, 64, 64)\n",
    "kp_variance = 0.01\n",
    "\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    "    # on_trace_ready=tensorboard_trace_handler(\"./log/kp2gaussian\"),\n",
    ") as prof:\n",
    "    with record_function(\"kp2gaussian\"):\n",
    "        kp2gaussian(kp_driving, spatial_size=spatial_size, kp_variance=kp_variance)\n",
    "prof.export_chrome_trace(\"./log/kp2gaussian.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time\n",
    "# start = time.time()\n",
    "# coordinate_grid = torch.randn((50, 15, 16, 64, 64, 3), device=\"cpu\")\n",
    "# print(time.time() - start)\n",
    "\n",
    "\n",
    "def make_coordinate_grid(spatial_size):\n",
    "    d, h, w = spatial_size\n",
    "    device=\"cpu\"\n",
    "    x = torch.arange(w, device=device)\n",
    "    y = torch.arange(h, device=device)\n",
    "    z = torch.arange(d, device=device)\n",
    "    print(x.device, y.device, z.device)\n",
    "    x = 2 * (x / (w - 1)) - 1\n",
    "    y = 2 * (y / (h - 1)) - 1\n",
    "    z = 2 * (z / (d - 1)) - 1\n",
    "    print(x.device, y.device, z.device)\n",
    "    yy = y.view(1, -1, 1).repeat(d, 1, w)\n",
    "    xx = x.view(1, 1, -1).repeat(d, h, 1)\n",
    "    zz = z.view(-1, 1, 1).repeat(1, h, w)\n",
    "    print(yy.device, xx.device, zz.device)\n",
    "    meshed = torch.cat([xx.unsqueeze_(3), yy.unsqueeze_(3), zz.unsqueeze_(3)], 3)\n",
    "    print(meshed.device)\n",
    "    return meshed\n",
    "\n",
    "# spatial_size = (16, 64, 64)\n",
    "# start = time.time()\n",
    "# make_coordinate_grid(spatial_size)\n",
    "# print(time.time() - start)\n",
    "\n",
    "def headpose_pred_to_degree(pred):  # slow\n",
    "    idx_tensor = torch.arange(pred.shape[1], device=pred.device)\n",
    "    # NOTE: doesn't require pred and idx_tensor to have the same dtype\n",
    "    degree = torch.sum(pred.softmax(1) * idx_tensor, 1) * 3 - 99\n",
    "    return degree\n",
    "\n",
    "# pred = torch.randn(50, 66).to(\"cpu\")\n",
    "# start = time.time()\n",
    "# headpose_pred_to_degree(pred)\n",
    "# print(time.time() - start)\n",
    "\n",
    "predictions = []\n",
    "for _ in range(4):\n",
    "    time.sleep(1)\n",
    "    predictions.append(torch.randn((1 * 1024**3), device=\"cuda:3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using safetensor as default\n",
      "Loading checkpoint from /home/80026129/PROJECTS/oncobot/src/sadtalker/checkpoints/SadTalker_V0.0.2_256.safetensors\n",
      "OcclusionAwareSPADEGenerator model size: 385.550 MB\n",
      "KPDetector model size: 160.013 MB\n",
      "HEEstimator model size: 115.643 MB\n",
      "MappingNet model size: 38.892 MB\n",
      "dtype: torch.float32 device: cuda:1 dp_device_ids: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.nn.parallel import DataParallel\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(os.path.abspath(\"\")).parents[2]))\n",
    "\n",
    "from src.sadtalker.src.utils.init_path import init_path\n",
    "from src.sadtalker.src.facerender.animate import AnimateFromCoeff\n",
    "from src.sadtalker.src.facerender.modules.generator import OcclusionAwareSPADEGenerator\n",
    "from src.sadtalker.src.facerender.modules.make_animation import keypoint_transformation\n",
    "\n",
    "image_size = 256\n",
    "image_preprocess = \"crop\"\n",
    "checkpoint_path = Path(os.path.abspath(\"\")).parents[2] / \"src/sadtalker/checkpoints\"\n",
    "gfpgan_path = Path(os.path.abspath(\"\")).parents[2] / \"src/sadtalker/gfpgan/weights\"\n",
    "config_path = Path(os.path.abspath(\"\")).parents[2] / \"src/sadtalker/src/config\"\n",
    "sadtalker_paths = init_path(\n",
    "    str(checkpoint_path),\n",
    "    str(gfpgan_path),\n",
    "    str(config_path),\n",
    "    image_size,\n",
    "    False,\n",
    "    image_preprocess,\n",
    ")\n",
    "\n",
    "batch_size = 30\n",
    "def generate(\n",
    "    source_image: torch.Tensor,\n",
    "    source_semantics: torch.Tensor,\n",
    "    target_semantics: torch.Tensor,\n",
    "    generator: (\n",
    "        OcclusionAwareSPADEGenerator | DataParallel[OcclusionAwareSPADEGenerator]\n",
    "    ),\n",
    "    device,\n",
    "    dtype,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        kp_canonical = {\n",
    "            \"value\": torch.rand((batch_size, 15, 3), device=device, dtype=dtype)\n",
    "        }\n",
    "        he_source = {\n",
    "            \"yaw\": torch.rand((batch_size, 66), device=device, dtype=dtype),\n",
    "            \"pitch\": torch.rand((batch_size, 66), device=device, dtype=dtype),\n",
    "            \"roll\": torch.rand((batch_size, 66), device=device, dtype=dtype),\n",
    "            \"t\": torch.rand((batch_size, 3), device=device, dtype=dtype),\n",
    "            \"exp\": torch.rand((batch_size, 45), device=device, dtype=dtype),\n",
    "        }\n",
    "        kp_source = keypoint_transformation(kp_canonical, he_source)\n",
    "\n",
    "        he_driving = {\n",
    "            \"yaw\": torch.rand((batch_size, 66), device=device, dtype=dtype),\n",
    "            \"pitch\": torch.rand((batch_size, 66), device=device, dtype=dtype),\n",
    "            \"roll\": torch.rand((batch_size, 66), device=device, dtype=dtype),\n",
    "            \"t\": torch.rand((batch_size, 3), device=device, dtype=dtype),\n",
    "            \"exp\": torch.rand((batch_size, 45), device=device, dtype=dtype),\n",
    "        }\n",
    "        kp_driving = keypoint_transformation(kp_canonical, he_driving)\n",
    "\n",
    "        # start = time.time()\n",
    "        out = generator(source_image, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        # print(\"generator time\", time.time() - start)\n",
    "        return out\n",
    "\n",
    "model = AnimateFromCoeff(\n",
    "    sadtalker_paths,\n",
    "    device=\"cuda:1\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "source_image = torch.rand(batch_size, 3, 256, 256)\n",
    "source_semantics = torch.rand(batch_size, 70, 27)\n",
    "target_semantics = torch.rand(batch_size, 8, 70, 27)\n",
    "source_image = source_image.type(model.dtype).to(model.device)\n",
    "source_semantics = source_semantics.type(model.dtype).to(model.device)\n",
    "target_semantics = target_semantics.type(model.dtype).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "warmup time 0.03161358833312988\n",
      "start\n",
      "end\n",
      "warmup time 2.487888813018799\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for _ in range(2):  # Warm-up iterations\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    print(\"start\", flush=True)\n",
    "    generate(\n",
    "        source_image,\n",
    "        source_semantics,\n",
    "        target_semantics,\n",
    "        model.generator,\n",
    "        model.device,\n",
    "        model.dtype,\n",
    "    )\n",
    "    print(\"end\", flush=True)    \n",
    "    torch.cuda.synchronize()\n",
    "    print(\"warmup time\", time.time() - start, flush=True)\n",
    "\n",
    "for _ in range(0):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    generate(\n",
    "        source_image,\n",
    "        source_semantics,\n",
    "        target_semantics,\n",
    "        model.generator,\n",
    "        model.device,\n",
    "        model.dtype,\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"time\", time.time() - start, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
